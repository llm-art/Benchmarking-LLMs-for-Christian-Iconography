{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load images with Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1864 images\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "# Open test.txt and read the lines\n",
    "with open('2_test.txt', 'r') as file:\n",
    "  test_items = file.read().splitlines()\n",
    "\n",
    "images = []\n",
    "\n",
    "for item in test_items:\n",
    "  image_path = os.path.join(os.pardir,'dataset', 'ArtDL', 'JPEGImages', f\"{item}.jpg\")\n",
    "  try:\n",
    "    image = Image.open(image_path)\n",
    "    images.append(image)\n",
    "  except Exception as e:\n",
    "    print(f\"Error loading image {image_path}: {e}\")\n",
    "\n",
    "print(f\"Loaded {len(images)} images\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test CLIP with these models:\n",
    "\n",
    "* openai/clip-vit-base-patch32\n",
    "* openai/clip-vit-base-patch16\n",
    "* openai/clip-vit-large-patch14\n",
    "\n",
    "\n",
    "Process the images and see their probability against classes.\n",
    "Use small batches (16 images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images: 1864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images: 100%|██████████| 1864/1864 [00:36<00:00, 51.11image/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities shape: torch.Size([1864, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, AutoModelForZeroShotImageClassification\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"PYDEVD_DISABLE_FILE_VALIDATION\"] = \"true\"\n",
    "\n",
    "model_name = \"clip-vit-base-patch32\"\n",
    "\n",
    "print(f\"Number of images: {len(images)}\")\n",
    "\n",
    "# Load the model and processor\n",
    "processor = AutoProcessor.from_pretrained(f'openai/{model_name}')\n",
    "model = AutoModelForZeroShotImageClassification.from_pretrained(f'openai/{model_name}')\n",
    "\n",
    "classes = [\n",
    "    (\"11H(ANTHONY OF PADUA)\", \"ANTHONY OF PADUA\"),\n",
    "    (\"11H(JOHN THE BAPTIST)\", \"JOHN THE BAPTIST\"),\n",
    "    (\"11H(PAUL)\", \"PAUL\"),\n",
    "    (\"11H(FRANCIS)\", \"FRANCIS OF ASSISI\"),\n",
    "    (\"11HH(MARY MAGDALENE)\", \"MARY MAGDALENE\"),\n",
    "    (\"11H(JEROME)\", \"JEROME\"),\n",
    "    (\"11H(DOMINIC)\", \"SAINT DOMINIC\"),\n",
    "    (\"11F(MARY)\", \"VIRGIN MARY\"),\n",
    "    (\"11H(PETER)\", \"PETER\"),\n",
    "    (\"11H(SEBASTIAN)\", \"SAINT SEBASTIAN\")\n",
    "]\n",
    "\n",
    "\n",
    "# Break images into smaller batches\n",
    "batch_size = 16\n",
    "images_batches = [images[i:i + batch_size] for i in range(0, len(images), batch_size)]\n",
    "\n",
    "all_probs = []\n",
    "with tqdm(total=len(images), desc=\"Processing Images\", unit=\"image\") as pbar:\n",
    "    for batch_index, batch in enumerate(images_batches):\n",
    "        try:\n",
    "            # Process the batch\n",
    "            inputs = processor(text=[cls[1] for cls in classes], images=batch, return_tensors=\"pt\", padding=True)\n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "            # Get probabilities for the batch\n",
    "            logits_per_image = outputs.logits_per_image  \n",
    "            batch_probs = logits_per_image.softmax(dim=1)\n",
    "            all_probs.append(batch_probs.detach())\n",
    "            \n",
    "            pbar.update(len(batch))\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch {batch_index + 1}: {e}\")\n",
    "            pbar.update(len(batch))\n",
    "\n",
    "# Get one tensor with all the probabilities\n",
    "all_probs = torch.cat(all_probs, dim=0)\n",
    "print(f\"Probabilities shape: {all_probs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "+-------------------+----+-----+------+\n",
      "|                   | TP | FP  |  FN  |\n",
      "+-------------------+----+-----+------+\n",
      "| ANTHONY OF PADUA  | 4  | 111 |  10  |\n",
      "| JOHN THE BAPTIST  | 22 | 107 |  77  |\n",
      "|       PAUL        | 0  |  3  |  52  |\n",
      "| FRANCIS OF ASSISI | 74 | 304 |  24  |\n",
      "|  MARY MAGDALENE   | 76 | 485 |  14  |\n",
      "|      JEROME       | 0  |  4  | 118  |\n",
      "|   SAINT DOMINIC   | 19 | 319 |  10  |\n",
      "|    VIRGIN MARY    | 72 |  0  | 1117 |\n",
      "|       PETER       | 0  |  0  | 119  |\n",
      "|  SAINT SEBASTIAN  | 50 | 214 |  6   |\n",
      "+-------------------+----+-----+------+\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tabulate\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "output_dir = f'../evaluations/{model_name}'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "probs = all_probs\n",
    "\n",
    "with open(os.path.join(os.pardir, '2_ground_truth.json'), 'r') as json_file:\n",
    "  ground_truth_data = json.load(json_file)\n",
    "ground_truth_dict = {item['item']: item['class'] for item in ground_truth_data}\n",
    "\n",
    "class_names = [cls[1] for cls in classes]\n",
    "\n",
    "# Create confusion matrix using ground truth and predicted classes\n",
    "y_true = [ground_truth_dict.get(os.path.basename(item.filename).split('.')[0]) for item in images]\n",
    "y_pred = [classes[all_probs[i].argmax().item()][1] for i in range(len(images))]\n",
    "y_true_indices = [class_names.index(cls) for cls in y_true]\n",
    "y_pred_indices = [class_names.index(cls) for cls in y_pred]\n",
    "cm = confusion_matrix(y_true_indices, y_pred_indices, labels=range(len(class_names)))\n",
    "\n",
    "# Populate confusion matrix dictionary\n",
    "confusion_matrices = {cls: {'TP': 0, 'FP': 0, 'FN': 0} for cls in class_names}\n",
    "for i, cls in enumerate(class_names):\n",
    "  confusion_matrices[cls]['TP'] = cm[i, i]\n",
    "  confusion_matrices[cls]['FP'] = cm[:, i].sum() - cm[i, i]\n",
    "  confusion_matrices[cls]['FN'] = cm[i, :].sum() - cm[i, i]\n",
    "\n",
    "# Store dataframe\n",
    "confusion_matrix_df = pd.DataFrame(confusion_matrices).T\n",
    "confusion_matrix_df = confusion_matrix_df[['TP', 'FP', 'FN']]\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(tabulate.tabulate(confusion_matrix_df, headers='keys', tablefmt='pretty'))\n",
    "confusion_matrix_df.to_csv(os.path.join(output_dir,'confusion_matrix.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics:\n",
      "+----+-------------------+-------------+-----------+--------+----------+-------------------+\n",
      "|    |    Class Name     | # of Images | Precision | Recall | F1 Score | Average Precision |\n",
      "+----+-------------------+-------------+-----------+--------+----------+-------------------+\n",
      "| 0  | ANTHONY OF PADUA  |     14      |   3.48%   | 28.57% |  6.20%   |       3.73%       |\n",
      "| 1  | FRANCIS OF ASSISI |     98      |  19.58%   | 75.51% |  31.09%  |      38.10%       |\n",
      "| 2  |      JEROME       |     118     |   0.00%   | 0.00%  |  0.00%   |      15.78%       |\n",
      "| 3  | JOHN THE BAPTIST  |     99      |  17.05%   | 22.22% |  19.30%  |      20.15%       |\n",
      "| 4  |  MARY MAGDALENE   |     90      |  13.55%   | 84.44% |  23.35%  |      60.57%       |\n",
      "| 5  |       PAUL        |     52      |   0.00%   | 0.00%  |  0.00%   |       5.24%       |\n",
      "| 6  |       PETER       |     119     |   0.00%   | 0.00%  |  0.00%   |      14.73%       |\n",
      "| 7  |   SAINT DOMINIC   |     29      |   5.62%   | 65.52% |  10.35%  |      22.02%       |\n",
      "| 8  |  SAINT SEBASTIAN  |     56      |  18.94%   | 89.29% |  31.25%  |      72.94%       |\n",
      "| 9  |    VIRGIN MARY    |    1189     |  100.00%  | 6.06%  |  11.42%  |      94.62%       |\n",
      "| 10 |       Mean        |      -      |  17.82%   | 37.16% |  13.30%  |      34.79%       |\n",
      "+----+-------------------+-------------+-----------+--------+----------+-------------------+\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, average_precision_score\n",
    "\n",
    "class_image_counts = {cls: 0 for cls in class_names}\n",
    "for item in y_true:\n",
    "  class_image_counts[item] += 1\n",
    "\n",
    "# Calculate precision for each class\n",
    "class_precisions = precision_score(y_true_indices, y_pred_indices, average=None, labels=range(len(class_names)), zero_division=0) * 100\n",
    "class_recalls = recall_score(y_true_indices, y_pred_indices, average=None, labels=range(len(class_names)), zero_division=0) * 100\n",
    "class_f1_scores = f1_score(y_true_indices, y_pred_indices, average=None, labels=range(len(class_names)), zero_division=0) * 100\n",
    "class_avg_precisions = average_precision_score(y_true_indices, all_probs, average=None) * 100\n",
    "\n",
    "# Store precision, recall, and f1 score for each class into a dataframe\n",
    "metrics_df = pd.DataFrame({\n",
    "  'Class Name': class_names,\n",
    "  '# of Images': [count for count in class_image_counts.values()],\n",
    "  'Precision': [f\"{p:.2f}%\" for p in class_precisions],\n",
    "  'Recall': [f\"{r:.2f}%\" for r in class_recalls],\n",
    "  'F1 Score': [f\"{f1:.2f}%\" for f1 in class_f1_scores],\n",
    "  'Average Precision': [f\"{ap:.2f}%\" for ap in class_avg_precisions]\n",
    "})\n",
    "\n",
    "# Reorder the dataframe based on the specified class order\n",
    "class_order = [\"ANTHONY OF PADUA\", \"FRANCIS OF ASSISI\", \"JEROME\", \"JOHN THE BAPTIST\", \"MARY MAGDALENE\", \"PAUL\", \"PETER\", \"SAINT DOMINIC\", \"SAINT SEBASTIAN\", \"VIRGIN MARY\"]\n",
    "metrics_df['Class Name'] = pd.Categorical(metrics_df['Class Name'], categories=class_order + [\"MEAN\"], ordered=True)\n",
    "metrics_df = metrics_df.sort_values('Class Name').reset_index(drop=True)\n",
    "\n",
    "# Add mean values to the dataframe\n",
    "mean_precision = precision_score(y_true_indices, y_pred_indices, average='macro', zero_division=0) * 100\n",
    "mean_recall = recall_score(y_true_indices, y_pred_indices, average='macro', zero_division=0) * 100\n",
    "mean_f1_score = f1_score(y_true_indices, y_pred_indices, average='macro', zero_division=0) * 100\n",
    "mean_avg_precision = average_precision_score(y_true_indices, all_probs, average='macro') * 100\n",
    "metrics_df.loc['10'] = ['Mean', '-', f\"{mean_precision:.2f}%\", f\"{mean_recall:.2f}%\", f\"{mean_f1_score:.2f}%\", f\"{mean_avg_precision:.2f}%\"]\n",
    "\n",
    "print(\"Metrics:\")\n",
    "print(tabulate.tabulate(metrics_df, headers='keys', tablefmt='pretty'))\n",
    "metrics_df.to_csv(os.path.join(output_dir,'metrics.csv'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
