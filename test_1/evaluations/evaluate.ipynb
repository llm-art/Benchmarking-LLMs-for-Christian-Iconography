{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models\n",
      "['clip-vit-base-patch16', 'siglip-base-patch16-512', 'clip-vit-large-patch14', 'clip-vit-base-patch32', 'siglip-large-patch16-384', 'siglip-so400m-patch14-384']\n",
      "\n",
      "Classes\n",
      "['ANTHONY OF PADUA', 'JOHN THE BAPTIST', 'PAUL', 'FRANCIS OF ASSISI', 'MARY MAGDALENE', 'JEROME', 'SAINT DOMINIC', 'VIRGIN MARY', 'PETER', 'SAINT SEBASTIAN']\n",
      "\n",
      "Ground truth loaded\n",
      "\n",
      "Test images loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Load models\n",
    "models = [name for name in os.listdir() if os.path.isdir(name)]\n",
    "print('Models')\n",
    "print(models)\n",
    "\n",
    "# Load classes\n",
    "with open(os.path.join(os.pardir, 'classes.txt'), 'r') as f:\n",
    "  classes = [tuple(line.strip().split(',')) for line in f]\n",
    "class_names = [cls[1] for cls in classes]\n",
    "print('\\nClasses')\n",
    "print(class_names)\n",
    "\n",
    "# Create ground truth dictionary\n",
    "with open(os.path.join(os.pardir, os.pardir, '2_ground_truth.json'), 'r') as json_file:\n",
    "  ground_truth_data = json.load(json_file)\n",
    "ground_truth_dict = {item['item']: item['class'] for item in ground_truth_data}\n",
    "print('\\nGround truth loaded')\n",
    "\n",
    "# Test images\n",
    "with open(os.path.join(os.pardir, os.pardir, '2_test.txt'), 'r') as file:\n",
    "  images = file.read().splitlines()\n",
    "print('\\nTest images loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: clip-vit-base-patch16\n",
      "Model: siglip-base-patch16-512\n",
      "Model: clip-vit-large-patch14\n",
      "Model: clip-vit-base-patch32\n",
      "Model: siglip-large-patch16-384\n",
      "Model: siglip-so400m-patch14-384\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import tabulate\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_trues = []\n",
    "y_preds = []\n",
    "all_probs = []\n",
    "\n",
    "for model_name in models:\n",
    "  output_dir = os.path.join(os.pardir,'evaluations',model_name)\n",
    "  os.makedirs(output_dir, exist_ok=True)\n",
    "  print(f\"Model: {model_name}\")\n",
    "\n",
    "  # Load model data\n",
    "  probs = torch.load(os.path.join(model_name, 'probs.pt'), weights_only=True)\n",
    "  all_probs.append(probs)\n",
    "\n",
    "  # Create confusion matrix using ground truth and predicted classes\n",
    "  y_true = [ground_truth_dict.get(item) for item in images]\n",
    "  y_pred = [classes[probs[i].argmax().item()][1] for i in range(len(images))]\n",
    "  y_true_indices = [class_names.index(cls) for cls in y_true]\n",
    "  y_pred_indices = [class_names.index(cls) for cls in y_pred]\n",
    "  \n",
    "  y_trues.append(y_true_indices)\n",
    "  y_preds.append(y_pred_indices)\n",
    "  \n",
    "  cm = confusion_matrix(y_true_indices, y_pred_indices, labels=range(len(class_names)))\n",
    "\n",
    "  confusion_matrix_df = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
    "  confusion_matrix_df.to_csv(os.path.join(output_dir, 'confusion_matrix.csv'))\n",
    "\n",
    "  plt.figure(figsize=(10, 8))\n",
    "  sns.heatmap(confusion_matrix_df, annot=True, fmt='d', cmap='Oranges', xticklabels=class_names, yticklabels=class_names)\n",
    "  plt.xlabel('Predicted classes')\n",
    "  plt.ylabel('Actual classes')\n",
    "  plt.title(f'Confusion Matrix for {model_name}')\n",
    "  plt.savefig(os.path.join(output_dir, 'confusion_matrix.png'), bbox_inches='tight')\n",
    "  plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executed clip-vit-base-patch16\n",
      "Executed siglip-base-patch16-512\n",
      "Executed clip-vit-large-patch14\n",
      "Executed clip-vit-base-patch32\n",
      "Executed siglip-large-patch16-384\n",
      "Executed siglip-so400m-patch14-384\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, average_precision_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "KEY_CLASS_NAME = 'class_name'\n",
    "KEY_NUM_IMAGES = '# of Images'\n",
    "KEY_PRECISION = 'Precision'\n",
    "KEY_RECALL = 'Recall'\n",
    "KEY_F1_SCORE = 'F1 Score'\n",
    "KEY_AVG_PRECISION = 'Average Precision'\n",
    "\n",
    "class_image_counts = {cls: 0 for cls in class_names}\n",
    "for item in y_true:\n",
    "  class_image_counts[item] += 1\n",
    "\n",
    "for i in range(0, len(models)):\n",
    "\n",
    "  y_true_indices = y_trues[i]\n",
    "  y_pred_indices = y_preds[i]\n",
    "  probs = all_probs[i]\n",
    "  \n",
    "  # one vs all encoding\n",
    "  y_true_one_hot = label_binarize(y_true_indices, classes=range(len(class_names)))\n",
    "\n",
    "  # Calculate metrics\n",
    "  class_precisions = precision_score(y_true_indices, y_pred_indices, average=None, labels=range(len(class_names)), zero_division=0) * 100\n",
    "  class_recalls = recall_score(y_true_indices, y_pred_indices, average=None, labels=range(len(class_names)), zero_division=0) * 100\n",
    "  class_f1_scores = f1_score(y_true_indices, y_pred_indices, average=None, labels=range(len(class_names)), zero_division=0) * 100\n",
    "  class_avg_precisions = average_precision_score(y_true_one_hot, probs, average=None) * 100\n",
    "\n",
    "  # Store precision, recall, and f1 score for each class into a dataframe\n",
    "  metrics_df = pd.DataFrame({\n",
    "      KEY_CLASS_NAME: class_names,\n",
    "      KEY_NUM_IMAGES: [count for count in class_image_counts.values()],\n",
    "      KEY_PRECISION: class_precisions,  \n",
    "      KEY_RECALL: class_recalls,        \n",
    "      KEY_F1_SCORE: class_f1_scores,    \n",
    "      KEY_AVG_PRECISION: class_avg_precisions \n",
    "  })\n",
    "\n",
    "  # Reorder the dataframe based on the specified class order\n",
    "  class_order = [\"ANTHONY OF PADUA\", \"FRANCIS OF ASSISI\", \"JEROME\", \"JOHN THE BAPTIST\", \"MARY MAGDALENE\", \"PAUL\", \"PETER\", \"SAINT DOMINIC\", \"SAINT SEBASTIAN\", \"VIRGIN MARY\"]\n",
    "  metrics_df[KEY_CLASS_NAME] = pd.Categorical(metrics_df[KEY_CLASS_NAME], categories=class_order + [\"MEAN\"], ordered=True)\n",
    "  metrics_df = metrics_df.sort_values(KEY_CLASS_NAME).reset_index(drop=True)\n",
    "\n",
    "  # Add mean values to the dataframe\n",
    "  mean_precision = precision_score(y_true_indices, y_pred_indices, average='macro', zero_division=0) * 100\n",
    "  mean_recall = recall_score(y_true_indices, y_pred_indices, average='macro', zero_division=0) * 100\n",
    "  mean_f1_score = f1_score(y_true_indices, y_pred_indices, average='macro', zero_division=0) * 100\n",
    "  mean_avg_precision = average_precision_score(y_true_indices, probs, average='macro') * 100\n",
    "  mean_values = {\n",
    "      KEY_CLASS_NAME: 'Mean (macro)',\n",
    "      KEY_NUM_IMAGES: '-',\n",
    "      KEY_PRECISION: mean_precision,\n",
    "      KEY_RECALL: mean_recall,\n",
    "      KEY_F1_SCORE: mean_f1_score,\n",
    "      KEY_AVG_PRECISION: mean_avg_precision\n",
    "  }\n",
    "  metrics_df = pd.concat([metrics_df, pd.DataFrame([mean_values])], ignore_index=True)\n",
    "\n",
    "  metrics_df[[KEY_PRECISION, KEY_RECALL, KEY_F1_SCORE, KEY_AVG_PRECISION]] = metrics_df[\n",
    "      [KEY_PRECISION, KEY_RECALL, KEY_F1_SCORE, KEY_AVG_PRECISION]\n",
    "  ].map(lambda x: f\"{x:.2f}%\")\n",
    "\n",
    "  metrics_path = os.path.join(os.pardir, 'evaluations', models[i], 'metrics.csv')\n",
    "  metrics_df.to_csv(metrics_path)\n",
    "  print(f\"Metrics stored in {metrics_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
