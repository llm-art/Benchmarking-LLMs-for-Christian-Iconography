{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file 'ArtDL.zip' already exists. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "download_url = \"https://zenodo.org/record/6473001/files/ArtDL.zip\"\n",
    "local_file_name = \"ArtDL.zip\"\n",
    "\n",
    "if os.path.exists(local_file_name):\n",
    "  print(f\"The file '{local_file_name}' already exists. Skipping download.\")\n",
    "\n",
    "else:\n",
    "  print(f\"Downloading the dataset from {download_url}...\")\n",
    "  \n",
    "  head_response = requests.head(download_url)\n",
    "  file_size = int(head_response.headers.get(\"content-length\", 0))\n",
    "  \n",
    "  response = requests.get(download_url, stream=True)\n",
    "  response.raise_for_status()\n",
    "  \n",
    "  with tqdm(total=file_size, unit=\"B\", unit_scale=True, desc=local_file_name) as pbar:\n",
    "    with open(local_file_name, \"wb\") as file:\n",
    "      for chunk in response.iter_content(chunk_size=8192):\n",
    "        file.write(chunk)\n",
    "        pbar.update(len(chunk))  # Update progress bar\n",
    "  \n",
    "  print(f\"Dataset downloaded and saved as '{local_file_name}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the zip file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting 'ArtDL.zip' to 'dataset'...\n",
      "Extraction complete. Files are saved in 'dataset'\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "zip_file = \"ArtDL.zip\"\n",
    "\n",
    "extract_dir = \"dataset\" \n",
    "\n",
    "if not os.path.exists(zip_file):\n",
    "    print(f\"The file '{zip_file}' does not exist. Please download it first.\")\n",
    "else:\n",
    "    print(f\"Extracting '{zip_file}' to '{extract_dir}'...\")\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_dir)\n",
    "    print(f\"Extraction complete. Files are saved in '{extract_dir}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check what is contained in the dataset. Print and store images per classes. Also, images may have multiple classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table of classes:\n",
      "+-------+------------------+-------------------+--------+------------------+----------------+------+-------+---------------+-----------------+-------------+\n",
      "|  set  | ANTHONY OF PADUA | FRANCIS OF ASSISI | JEROME | JOHN THE BAPTIST | MARY MAGDALENE | PAUL | PETER | SAINT DOMINIC | SAINT SEBASTIAN | VIRGIN MARY |\n",
      "+-------+------------------+-------------------+--------+------------------+----------------+------+-------+---------------+-----------------+-------------+\n",
      "| train |       170        |       1220        |  1285  |       1497       |      1949      | 754  | 1471  |      387      |       628       |    15566    |\n",
      "|  val  |        22        |        144        |  151   |       154        |      235       |  91  |  176  |      47       |       74        |    1920     |\n",
      "| test  |        22        |        142        |  154   |       159        |      238       |  94  |  178  |      47       |       75        |    1913     |\n",
      "+-------+------------------+-------------------+--------+------------------+----------------+------+-------+---------------+-----------------+-------------+\n",
      "Table of classes with weights:\n",
      "+-------+------------------+-------------------+--------+------------------+----------------+------+-------+---------------+-----------------+-------------+\n",
      "|  set  | ANTHONY OF PADUA | FRANCIS OF ASSISI | JEROME | JOHN THE BAPTIST | MARY MAGDALENE | PAUL | PETER | SAINT DOMINIC | SAINT SEBASTIAN | VIRGIN MARY |\n",
      "+-------+------------------+-------------------+--------+------------------+----------------+------+-------+---------------+-----------------+-------------+\n",
      "| train |       140        |        993        |  1098  |       1200       |      1329      | 559  | 1195  |      306      |       538       |    14312    |\n",
      "|  val  |        18        |        118        |  130   |       119        |      159       |  66  |  143  |      37       |       64        |    1770     |\n",
      "| test  |        18        |        115        |  132   |       123        |      162       |  69  |  145  |      36       |       64        |    1763     |\n",
      "+-------+------------------+-------------------+--------+------------------+----------------+------+-------+---------------+-----------------+-------------+\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tabulate\n",
    "\n",
    "csv_file_path = 'dataset/ArtDL/ArtDL.csv'\n",
    "\n",
    "classes = [\n",
    "    (\"11H(ANTONY OF PADUA)\", \"ANTHONY OF PADUA\"),\n",
    "    (\"11H(JOHN THE BAPTIST)\", \"JOHN THE BAPTIST\"),\n",
    "    (\"11H(PAUL)\", \"PAUL\"),\n",
    "    (\"11H(FRANCIS)\", \"FRANCIS OF ASSISI\"),\n",
    "    (\"11HH(MARY MAGDALENE)\", \"MARY MAGDALENE\"),\n",
    "    (\"11H(JEROME)\", \"JEROME\"),\n",
    "    (\"11H(DOMINIC)\", \"SAINT DOMINIC\"),\n",
    "    (\"11F(MARY)\", \"VIRGIN MARY\"),\n",
    "    (\"11H(PETER)\", \"PETER\"),\n",
    "    (\"11H(SEBASTIAN)\", \"SAINT SEBASTIAN\")\n",
    "]\n",
    "\n",
    "def organize_df(df):\n",
    "  column_mapping = {cls[0]: cls[1] for cls in classes}\n",
    "  df = df.rename(columns=column_mapping).set_index(\"set\")[sorted(column_mapping.values())].loc[[\"train\", \"val\", \"test\"]]\n",
    "  return df\n",
    "\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "columns_to_keep = [cls[0] for cls in classes]\n",
    "df = df[columns_to_keep + ['item', 'set']]\n",
    "\n",
    "df_normalized = df.copy()\n",
    "df_normalized = df.drop(columns=['item']).groupby('set').sum().reset_index()\n",
    "df_normalized = organize_df(df_normalized)\n",
    "\n",
    "print(\"Table of classes:\")\n",
    "print(tabulate.tabulate(df_normalized, headers='keys', tablefmt='pretty'))\n",
    "df_normalized.to_csv('1_ArtDL_classes.csv')\n",
    "df[columns_to_keep] = df[columns_to_keep].astype(float)\n",
    "\n",
    "# Weight the classes based on the number of items in each set\n",
    "for index, row in df.iterrows():\n",
    "  count_ones = row[columns_to_keep].sum()\n",
    "  if count_ones > 0:\n",
    "    df.loc[index, columns_to_keep] = row[columns_to_keep] / count_ones\n",
    "  \n",
    "df = df.drop(columns=['item']).groupby('set').sum().reset_index()\n",
    "\n",
    "df[columns_to_keep] = df[columns_to_keep].astype(int)\n",
    "\n",
    "df = organize_df(df)\n",
    "print(\"Table of classes with weights:\")\n",
    "print(tabulate.tabulate(df, headers='keys', tablefmt='pretty'))\n",
    "df.to_csv('1_ArtDL_classes_weighted.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the list of images used for testing purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All items in test.txt exist in ArtDL.csv\n",
      "All image files exist in JPEGImages folder\n",
      "Number of rows in test.txt: 1864\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Download the test.txt file\n",
    "test_txt_url = \"https://raw.githubusercontent.com/iFede94/ArtDL/refs/heads/main/sets/test.txt\"\n",
    "test_txt_file = \"2_test.txt\"\n",
    "\n",
    "response = requests.get(test_txt_url)\n",
    "with open(test_txt_file, 'wb') as file:\n",
    "  file.write(response.content)\n",
    "\n",
    "with open(test_txt_file, 'r') as file:\n",
    "  test_items = file.read().splitlines()\n",
    "\n",
    "csv_file_path = 'dataset/ArtDL/ArtDL.csv'\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "missing_items = []\n",
    "for item in test_items:\n",
    "  if df[df['item'] == item].empty:\n",
    "    missing_items.append(item)\n",
    "\n",
    "if missing_items:\n",
    "  print(\"Missing items:\")\n",
    "  for missing in missing_items:\n",
    "    print(missing)\n",
    "else:\n",
    "  print(\"All items in test.txt exist in ArtDL.csv\")\n",
    "\n",
    "  \n",
    "# Create ground truth file\n",
    "image_dir = \"dataset/ArtDL/JPEGImages\"\n",
    "missing_files = []\n",
    "\n",
    "for item in test_items:\n",
    "  file_name = f\"{item}.jpg\"\n",
    "  if not os.path.exists(os.path.join(image_dir, file_name)):\n",
    "    missing_files.append(file_name)\n",
    "\n",
    "if missing_files:\n",
    "  print(\"Missing image files:\")\n",
    "  for missing in missing_files:\n",
    "    print(missing)\n",
    "else:\n",
    "  print(\"All image files exist in JPEGImages folder\")\n",
    "\n",
    "  num_rows = len(test_items)\n",
    "  print(f\"Number of rows in test.txt: {num_rows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth data has been saved to 2_ground_truth.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "classes = [\n",
    "    (\"11H(ANTONY OF PADUA)\", \"ANTHONY OF PADUA\"),\n",
    "    (\"11H(JOHN THE BAPTIST)\", \"JOHN THE BAPTIST\"),\n",
    "    (\"11H(PAUL)\", \"PAUL\"),\n",
    "    (\"11H(FRANCIS)\", \"FRANCIS OF ASSISI\"),\n",
    "    (\"11HH(MARY MAGDALENE)\", \"MARY MAGDALENE\"),\n",
    "    (\"11H(JEROME)\", \"JEROME\"),\n",
    "    (\"11H(DOMINIC)\", \"SAINT DOMINIC\"),\n",
    "    (\"11F(MARY)\", \"VIRGIN MARY\"),\n",
    "    (\"11H(PETER)\", \"PETER\"),\n",
    "    (\"11H(SEBASTIAN)\", \"SAINT SEBASTIAN\")\n",
    "]\n",
    "\n",
    "image_dir = \"dataset/ArtDL/JPEGImages\"\n",
    "json_file_path = \"2_ground_truth.json\"\n",
    "csv_file_path = \"dataset/ArtDL/ArtDL.csv\"\n",
    "test_file = \"2_test.txt\"\n",
    "\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "ground_truth_data = []\n",
    "\n",
    "with open(test_file, 'r') as file:\n",
    "  test_items = file.read().splitlines()\n",
    "\n",
    "# Process each image in the test file\n",
    "for item in test_items:\n",
    "  row = df[df['item'] == item]\n",
    "  if row.empty:\n",
    "    print(f\"Warning: No matching row found in CSV for item '{item}'. Skipping...\")\n",
    "    continue\n",
    "  \n",
    "  row = row.iloc[0]\n",
    "\n",
    "  # Find the column that is 1 and is in the classes list\n",
    "  for cls in classes:\n",
    "    if row[cls[0]] == 1:\n",
    "      ground_truth_data.append({\n",
    "        \"item\": item,\n",
    "        \"class\": cls[1]\n",
    "      })\n",
    "      break\n",
    "\n",
    "with open(json_file_path, 'w') as json_file:\n",
    "  json.dump(ground_truth_data, json_file, indent=4)\n",
    "\n",
    "print(f\"Ground truth data has been saved to {json_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load images with Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1864 images\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "# Open test.txt and read the lines\n",
    "with open('2_test.txt', 'r') as file:\n",
    "  test_items = file.read().splitlines()\n",
    "\n",
    "images = []\n",
    "\n",
    "for item in test_items:\n",
    "  image_path = os.path.join('dataset/ArtDL/JPEGImages', f\"{item}.jpg\")\n",
    "  try:\n",
    "    image = Image.open(image_path)\n",
    "    images.append(image)\n",
    "  except Exception as e:\n",
    "    print(f\"Error loading image {image_path}: {e}\")\n",
    "\n",
    "print(f\"Loaded {len(images)} images\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test CLIP with these models:\n",
    "\n",
    "openai/clip-vit-base-patch32\n",
    "openai/clip-vit-base-patch16\n",
    "openai/clip-vit-large-patch14\n",
    "\n",
    "\n",
    "Process the images and see their probability against classes.\n",
    "Use small batches (16 images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images: 1864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images: 100%|██████████| 1864/1864 [08:48<00:00,  3.53image/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities shape: torch.Size([1864, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, AutoModelForZeroShotImageClassification\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"PYDEVD_DISABLE_FILE_VALIDATION\"] = \"true\"\n",
    "\n",
    "model_name = \"clip-vit-large-patch14\"\n",
    "\n",
    "# limit for testing purposes\n",
    "#limit = 16 * 20\n",
    "#images = images[:limit]\n",
    "\n",
    "print(f\"Number of images: {len(images)}\")\n",
    "\n",
    "# Load the model and processor\n",
    "processor = AutoProcessor.from_pretrained(f'openai/{model_name}')\n",
    "model = AutoModelForZeroShotImageClassification.from_pretrained(f'openai/{model_name}')\n",
    "\n",
    "classes = [\n",
    "    (\"11H(ANTHONY OF PADUA)\", \"ANTHONY OF PADUA\"),\n",
    "    (\"11H(JOHN THE BAPTIST)\", \"JOHN THE BAPTIST\"),\n",
    "    (\"11H(PAUL)\", \"PAUL\"),\n",
    "    (\"11H(FRANCIS)\", \"FRANCIS OF ASSISI\"),\n",
    "    (\"11HH(MARY MAGDALENE)\", \"MARY MAGDALENE\"),\n",
    "    (\"11H(JEROME)\", \"JEROME\"),\n",
    "    (\"11H(DOMINIC)\", \"SAINT DOMINIC\"),\n",
    "    (\"11F(MARY)\", \"VIRGIN MARY\"),\n",
    "    (\"11H(PETER)\", \"PETER\"),\n",
    "    (\"11H(SEBASTIAN)\", \"SAINT SEBASTIAN\")\n",
    "]\n",
    "\n",
    "\n",
    "# Break images into smaller batches\n",
    "batch_size = 16\n",
    "images_batches = [images[i:i + batch_size] for i in range(0, len(images), batch_size)]\n",
    "\n",
    "all_probs = []\n",
    "with tqdm(total=len(images), desc=\"Processing Images\", unit=\"image\") as pbar:\n",
    "    for batch_index, batch in enumerate(images_batches):\n",
    "        try:\n",
    "            # Process the batch\n",
    "            inputs = processor(text=[cls[1] for cls in classes], images=batch, return_tensors=\"pt\", padding=True)\n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "            # Get probabilities for the batch\n",
    "            logits_per_image = outputs.logits_per_image  \n",
    "            batch_probs = logits_per_image.softmax(dim=1)\n",
    "            all_probs.append(batch_probs.detach())\n",
    "            \n",
    "            pbar.update(len(batch))\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch {batch_index + 1}: {e}\")\n",
    "            pbar.update(len(batch))\n",
    "\n",
    "# Get one tensor with all the probabilities\n",
    "all_probs = torch.cat(all_probs, dim=0)\n",
    "print(f\"Probabilities shape: {all_probs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Evaluation:\n",
      "+----+-------------------+---------------+-----------+--------+----------+-------------------+\n",
      "|    |    Class Name     | N Test Images | Precision | Recall | F1-Score | Average Precision |\n",
      "+----+-------------------+---------------+-----------+--------+----------+-------------------+\n",
      "| 0  | ANTHONY OF PADUA  |      14       |   4.71%   | 57.14% |  8.70%   |      11.41%       |\n",
      "| 1  | FRANCIS OF ASSISI |      98       |  28.97%   | 63.27% |  39.74%  |      35.69%       |\n",
      "| 2  |      JEROME       |      118      |   0.00%   | 0.00%  |  0.00%   |      14.76%       |\n",
      "| 3  | JOHN THE BAPTIST  |      99       |  23.79%   | 69.70% |  35.48%  |      28.37%       |\n",
      "| 4  |  MARY MAGDALENE   |      90       |  14.11%   | 88.89% |  24.35%  |      49.12%       |\n",
      "| 5  |       PAUL        |      52       |   0.00%   | 0.00%  |  0.00%   |      11.87%       |\n",
      "| 6  |       PETER       |      119      |   0.00%   | 0.00%  |  0.00%   |      14.97%       |\n",
      "| 7  |   SAINT DOMINIC   |      29       |   8.30%   | 65.52% |  14.73%  |      17.49%       |\n",
      "| 8  |  SAINT SEBASTIAN  |      56       |  25.41%   | 82.14% |  38.82%  |      65.63%       |\n",
      "| 9  |    VIRGIN MARY    |     1189      |  100.00%  | 17.58% |  29.90%  |      96.76%       |\n",
      "| 10 |       MEAN        |               |  20.53%   | 44.42% |  19.17%  |      34.61%       |\n",
      "+----+-------------------+---------------+-----------+--------+----------+-------------------+\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tabulate\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "probs = all_probs\n",
    "\n",
    "with open(json_file_path, 'r') as json_file:\n",
    "  ground_truth_data = json.load(json_file)\n",
    "ground_truth_dict = {item['item']: item['class'] for item in ground_truth_data}\n",
    "\n",
    "# Initialize confusion matrix\n",
    "confusion_matrices = {cls[1]: {'TP': 0, 'FP': 0, 'FN': 0} for cls in classes}\n",
    "results_df = pd.DataFrame()\n",
    "\n",
    "for i, item in enumerate(images):\n",
    "  class_probs = all_probs[i]\n",
    "  predicted_class = classes[class_probs.argmax().item()][1]\n",
    "  true_class = ground_truth_dict.get(test_items[i], None)\n",
    "  \n",
    "  results_df = pd.concat([results_df, pd.DataFrame([{\n",
    "      'File Name': os.path.basename(item.filename).split('.')[0],\n",
    "      'Predicted Class': predicted_class,\n",
    "      'True Class': true_class\n",
    "  }])], ignore_index=True)\n",
    "\n",
    "  for cls in classes:\n",
    "    class_name = cls[1]\n",
    "    if predicted_class == class_name and true_class == class_name:\n",
    "      confusion_matrices[class_name]['TP'] += 1\n",
    "    elif predicted_class == class_name and true_class != class_name:\n",
    "      confusion_matrices[class_name]['FP'] += 1\n",
    "    elif predicted_class != class_name and true_class == class_name:\n",
    "      confusion_matrices[class_name]['FN'] += 1\n",
    "\n",
    "# Store results if needed\n",
    "results_df.to_csv('3_checked.csv')\n",
    "\n",
    "performances_evaluation = pd.DataFrame(columns=[\"Class Name\", \"N Test Images\", \"Precision\", \"Recall\", \"F1-Score\", \"Average Precision\"])\n",
    "class_image_counts = {cls[1]: 0 for cls in classes}\n",
    "\n",
    "for cls in classes:\n",
    "  class_name = cls[1]\n",
    "  tp = confusion_matrices[class_name]['TP']\n",
    "  fp = confusion_matrices[class_name]['FP']\n",
    "  fn = confusion_matrices[class_name]['FN']\n",
    "  \n",
    "  precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "  recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "  f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "  \n",
    "  class_image_counts[class_name] = sum(1 for item in images if ground_truth_dict.get(os.path.basename(item.filename).split('.')[0]) == class_name)\n",
    "  \n",
    "  # Calculate average precision\n",
    "  y_true = [1 if ground_truth_dict.get(os.path.basename(item.filename).split('.')[0]) == class_name else 0 for item in images]\n",
    "  class_i = next(i for i, cls in enumerate(classes) if cls[1] == class_name)\n",
    "  y_scores = [probs[i][class_i].item() for i in range(len(images))]\n",
    "  avg_precision = average_precision_score(y_true, y_scores)\n",
    "  \n",
    "  performances_evaluation = pd.concat([performances_evaluation, pd.DataFrame([{\n",
    "      \"Class Name\": class_name,\n",
    "      \"N Test Images\": class_image_counts[class_name],\n",
    "      \"Precision\": f\"{precision:.2%}\",\n",
    "      \"Recall\": f\"{recall:.2%}\",\n",
    "      \"F1-Score\": f\"{f1_score:.2%}\",\n",
    "      \"Average Precision\": f\"{avg_precision:.2%}\"\n",
    "  }])], ignore_index=True)\n",
    "\n",
    "# Calculate mean values\n",
    "mean_precision = performances_evaluation[\"Precision\"].str.rstrip('%').astype(float).mean() / 100\n",
    "mean_recall = performances_evaluation[\"Recall\"].str.rstrip('%').astype(float).mean() / 100\n",
    "mean_f1_score = performances_evaluation[\"F1-Score\"].str.rstrip('%').astype(float).mean() / 100\n",
    "mean_avg_precision = performances_evaluation[\"Average Precision\"].str.rstrip('%').astype(float).mean() / 100\n",
    "\n",
    "performances_evaluation = pd.concat([performances_evaluation, pd.DataFrame([{\n",
    "  \"Class Name\": \"MEAN\",\n",
    "  \"N Test Images\": \"\",\n",
    "  \"Precision\": f\"{mean_precision:.2%}\",\n",
    "  \"Recall\": f\"{mean_recall:.2%}\",\n",
    "  \"F1-Score\": f\"{mean_f1_score:.2%}\",\n",
    "  \"Average Precision\": f\"{mean_avg_precision:.2%}\"\n",
    "}])], ignore_index=True)\n",
    "\n",
    "# Reorder the dataframe based on the specified class order\n",
    "class_order = [\"ANTHONY OF PADUA\", \"FRANCIS OF ASSISI\", \"JEROME\", \"JOHN THE BAPTIST\", \"MARY MAGDALENE\", \"PAUL\", \"PETER\", \"SAINT DOMINIC\", \"SAINT SEBASTIAN\", \"VIRGIN MARY\"]\n",
    "performances_evaluation['Class Name'] = pd.Categorical(performances_evaluation['Class Name'], categories=class_order + [\"MEAN\"], ordered=True)\n",
    "performances_evaluation = performances_evaluation.sort_values('Class Name').reset_index(drop=True)\n",
    "\n",
    "os.makedirs('evaluations', exist_ok=True)\n",
    "\n",
    "print(\"Performance Evaluation:\")\n",
    "print(tabulate.tabulate(performances_evaluation, headers=\"keys\", tablefmt=\"pretty\"))\n",
    "performances_evaluation.to_csv(os.path.join('evaluations', f'{model_name}.csv'), index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
