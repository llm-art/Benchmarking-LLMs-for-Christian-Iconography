{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading the dataset from https://zenodo.org/record/6473001/files/ArtDL.zip...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ArtDL.zip: 3.60GB [03:00, 19.9MB/s]                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset downloaded and saved as 'ArtDL.zip'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "download_url = \"https://zenodo.org/record/6473001/files/ArtDL.zip\"\n",
    "local_file_name = \"ArtDL.zip\"\n",
    "\n",
    "if os.path.exists(local_file_name):\n",
    "  print(f\"The file '{local_file_name}' already exists. Skipping download.\")\n",
    "\n",
    "else:\n",
    "  print(f\"Downloading the dataset from {download_url}...\")\n",
    "  \n",
    "  head_response = requests.head(download_url)\n",
    "  file_size = int(head_response.headers.get(\"content-length\", 0))\n",
    "  \n",
    "  response = requests.get(download_url, stream=True)\n",
    "  response.raise_for_status()\n",
    "  \n",
    "  with tqdm(total=file_size, unit=\"B\", unit_scale=True, desc=local_file_name) as pbar:\n",
    "    with open(local_file_name, \"wb\") as file:\n",
    "      for chunk in response.iter_content(chunk_size=8192):\n",
    "        file.write(chunk)\n",
    "        pbar.update(len(chunk))  # Update progress bar\n",
    "  \n",
    "  print(f\"Dataset downloaded and saved as '{local_file_name}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting 'ArtDL.zip' to 'dataset'...\n",
      "Extraction complete. Files are saved in 'dataset'\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "zip_file = \"ArtDL.zip\"\n",
    "\n",
    "extract_dir = \"dataset\" \n",
    "\n",
    "if not os.path.exists(zip_file):\n",
    "    print(f\"The file '{zip_file}' does not exist. Please download it first.\")\n",
    "else:\n",
    "    print(f\"Extracting '{zip_file}' to '{extract_dir}'...\")\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_dir)\n",
    "    print(f\"Extraction complete. Files are saved in '{extract_dir}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table of classes:\n",
      "+-------+------------------+-------------------+--------+------------------+----------------+------+-------+---------------+-----------------+-------------+\n",
      "|  set  | ANTHONY OF PADUA | FRANCIS OF ASSISI | JEROME | JOHN THE BAPTIST | MARY MAGDALENE | PAUL | PETER | SAINT DOMINIC | SAINT SEBASTIAN | VIRGIN MARY |\n",
      "+-------+------------------+-------------------+--------+------------------+----------------+------+-------+---------------+-----------------+-------------+\n",
      "| train |       170        |       1220        |  1285  |       1497       |      1949      | 754  | 1471  |      387      |       628       |    15566    |\n",
      "|  val  |        22        |        144        |  151   |       154        |      235       |  91  |  176  |      47       |       74        |    1920     |\n",
      "| test  |        22        |        142        |  154   |       159        |      238       |  94  |  178  |      47       |       75        |    1913     |\n",
      "+-------+------------------+-------------------+--------+------------------+----------------+------+-------+---------------+-----------------+-------------+\n",
      "Table of classes with weights:\n",
      "+-------+------------------+-------------------+--------+------------------+----------------+------+-------+---------------+-----------------+-------------+\n",
      "|  set  | ANTHONY OF PADUA | FRANCIS OF ASSISI | JEROME | JOHN THE BAPTIST | MARY MAGDALENE | PAUL | PETER | SAINT DOMINIC | SAINT SEBASTIAN | VIRGIN MARY |\n",
      "+-------+------------------+-------------------+--------+------------------+----------------+------+-------+---------------+-----------------+-------------+\n",
      "| train |       140        |        993        |  1098  |       1200       |      1329      | 559  | 1195  |      306      |       538       |    14312    |\n",
      "|  val  |        18        |        118        |  130   |       119        |      159       |  66  |  143  |      37       |       64        |    1770     |\n",
      "| test  |        18        |        115        |  132   |       123        |      162       |  69  |  145  |      36       |       64        |    1763     |\n",
      "+-------+------------------+-------------------+--------+------------------+----------------+------+-------+---------------+-----------------+-------------+\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tabulate\n",
    "\n",
    "csv_file_path = 'dataset/ArtDL/ArtDL.csv'\n",
    "\n",
    "classes = [\n",
    "  (\"11H(ANTONY OF PADUA)\", \"ANTHONY OF PADUA\"),\n",
    "  (\"11H(JOHN THE BAPTIST)\", \"JOHN THE BAPTIST\"),\n",
    "  (\"11H(PAUL)\", \"PAUL\"),\n",
    "  (\"11H(FRANCIS)\", \"FRANCIS OF ASSISI\"),\n",
    "  (\"11HH(MARY MAGDALENE)\", \"MARY MAGDALENE\"),\n",
    "  (\"11H(JEROME)\", \"JEROME\"),\n",
    "  (\"11H(DOMINIC)\", \"SAINT DOMINIC\"),\n",
    "  (\"11F(MARY)\", \"VIRGIN MARY\"),\n",
    "  (\"11H(PETER)\", \"PETER\"),\n",
    "  (\"11H(SEBASTIAN)\", \"SAINT SEBASTIAN\")\n",
    "]\n",
    "\n",
    "# Store classes in a file\n",
    "with open('classes.txt', 'w') as file:\n",
    "  for cls in classes:\n",
    "    file.write(f\"{cls[0]},{cls[1]}\\n\")\n",
    "\n",
    "def organize_df(df):\n",
    "  column_mapping = {cls[0]: cls[1] for cls in classes}\n",
    "  df = df.rename(columns=column_mapping).set_index(\"set\")[sorted(column_mapping.values())].loc[[\"train\", \"val\", \"test\"]]\n",
    "  return df\n",
    "\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "columns_to_keep = [cls[0] for cls in classes]\n",
    "df = df[columns_to_keep + ['item', 'set']]\n",
    "\n",
    "df_normalized = df.copy()\n",
    "df_normalized = df.drop(columns=['item']).groupby('set').sum().reset_index()\n",
    "df_normalized = organize_df(df_normalized)\n",
    "\n",
    "print(\"Table of classes:\")\n",
    "print(tabulate.tabulate(df_normalized, headers='keys', tablefmt='pretty'))\n",
    "df_normalized.to_csv('1_ArtDL_classes.csv')\n",
    "df[columns_to_keep] = df[columns_to_keep].astype(float)\n",
    "\n",
    "# Weight the classes based on the number of items in each set\n",
    "for index, row in df.iterrows():\n",
    "  count_ones = row[columns_to_keep].sum()\n",
    "  if count_ones > 0:\n",
    "    df.loc[index, columns_to_keep] = row[columns_to_keep] / count_ones\n",
    "  \n",
    "df = df.drop(columns=['item']).groupby('set').sum().reset_index()\n",
    "\n",
    "df[columns_to_keep] = df[columns_to_keep].astype(int)\n",
    "\n",
    "df = organize_df(df)\n",
    "print(\"Table of classes with weights:\")\n",
    "print(tabulate.tabulate(df, headers='keys', tablefmt='pretty'))\n",
    "df.to_csv('1_ArtDL_classes_weighted.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All items in test.txt exist in ArtDL.csv\n",
      "All image files exist in JPEGImages folder\n",
      "Number of rows in test.txt: 1864\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Download the test.txt file\n",
    "test_txt_url = \"https://raw.githubusercontent.com/iFede94/ArtDL/refs/heads/main/sets/test.txt\"\n",
    "test_txt_file = \"2_test.txt\"\n",
    "\n",
    "response = requests.get(test_txt_url)\n",
    "with open(test_txt_file, 'wb') as file:\n",
    "  file.write(response.content)\n",
    "\n",
    "with open(test_txt_file, 'r') as file:\n",
    "  test_items = file.read().splitlines()\n",
    "\n",
    "csv_file_path = 'dataset/ArtDL/ArtDL.csv'\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "missing_items = []\n",
    "for item in test_items:\n",
    "  if df[df['item'] == item].empty:\n",
    "    missing_items.append(item)\n",
    "\n",
    "if missing_items:\n",
    "  print(\"Missing items:\")\n",
    "  for missing in missing_items:\n",
    "    print(missing)\n",
    "else:\n",
    "  print(\"All items in test.txt exist in ArtDL.csv\")\n",
    "\n",
    "  \n",
    "# Create ground truth file\n",
    "image_dir = \"dataset/ArtDL/JPEGImages\"\n",
    "missing_files = []\n",
    "\n",
    "for item in test_items:\n",
    "  file_name = f\"{item}.jpg\"\n",
    "  if not os.path.exists(os.path.join(image_dir, file_name)):\n",
    "    missing_files.append(file_name)\n",
    "\n",
    "if missing_files:\n",
    "  print(\"Missing image files:\")\n",
    "  for missing in missing_files:\n",
    "    print(missing)\n",
    "else:\n",
    "  print(\"All image files exist in JPEGImages folder\")\n",
    "\n",
    "  num_rows = len(test_items)\n",
    "  print(f\"Number of rows in test.txt: {num_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth data has been saved to 2_ground_truth.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "with open('classes.txt', 'r') as file:\n",
    "  classes = [tuple(line.strip().split(',')) for line in file]\n",
    "\n",
    "image_dir = \"dataset/ArtDL/JPEGImages\"\n",
    "json_file_path = \"2_ground_truth.json\"\n",
    "csv_file_path = \"dataset/ArtDL/ArtDL.csv\"\n",
    "test_file = \"2_test.txt\"\n",
    "\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "ground_truth_data = []\n",
    "\n",
    "with open(test_file, 'r') as file:\n",
    "  test_items = file.read().splitlines()\n",
    "\n",
    "# Process each image in the test file\n",
    "for item in test_items:\n",
    "  row = df[df['item'] == item]\n",
    "  if row.empty:\n",
    "    print(f\"Warning: No matching row found in CSV for item '{item}'. Skipping...\")\n",
    "    continue\n",
    "  \n",
    "  row = row.iloc[0]\n",
    "\n",
    "  # Find the column that is 1 and is in the classes list\n",
    "  for cls in classes:\n",
    "    if row[cls[0]] == 1:\n",
    "      ground_truth_data.append({\n",
    "        \"item\": item,\n",
    "        \"class\": cls[0]\n",
    "      })\n",
    "      break\n",
    "\n",
    "with open(json_file_path, 'w') as json_file:\n",
    "  json.dump(ground_truth_data, json_file, indent=4)\n",
    "\n",
    "print(f\"Ground truth data has been saved to {json_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
