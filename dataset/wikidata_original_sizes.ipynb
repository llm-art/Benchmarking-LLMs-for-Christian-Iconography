{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikidata Image Download with Original Sizes\n",
    "\n",
    "This notebook downloads images from Wikidata while preserving their original sizes and aspect ratios. It also tracks image statistics to compare with the target values from the paper (Width: 778.84 ± 198.74, Height: 669.36 ± 174.18)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "import logging\n",
    "from PIL import Image\n",
    "\n",
    "# Increase PIL threshold to match our max_pixels value\n",
    "Image.MAX_IMAGE_PIXELS = 178956970\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    filename='image_download.log',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "wikidata_dir = os.path.join(os.getcwd(), 'wikidata')\n",
    "os.makedirs(wikidata_dir, exist_ok=True)\n",
    "\n",
    "# Define the SPARQL endpoint and the query\n",
    "sparql = SPARQLWrapper(\"https://query.wikidata.org/sparql\")\n",
    "query = \"\"\"\n",
    "SELECT ?painting ?image ?iconclass WHERE {\n",
    "  ?painting wdt:P31 wd:Q3305213;        # instance of painting\n",
    "           wdt:P1257 ?iconclass.        # has an Iconclass code\n",
    "  ?painting wdt:P18 ?image.             # image filename if available\n",
    "  FILTER(strstarts(?iconclass, '11H'))\n",
    "}\n",
    "\"\"\"\n",
    "sparql.setQuery(query)\n",
    "sparql.setReturnFormat(JSON)\n",
    "\n",
    "# Execute the query and convert the result to a pandas DataFrame\n",
    "sparql.setRequestMethod('GET')\n",
    "results = sparql.query().convert()\n",
    "data = results['results']['bindings']\n",
    "\n",
    "# Extract the relevant fields and store them in a list of dictionaries\n",
    "data_list = []\n",
    "for item in data:\n",
    "  data_list.append({\n",
    "    'painting': item['painting']['value'],\n",
    "    'image': item['image']['value'],\n",
    "    'iconclass': item['iconclass']['value']\n",
    "  })\n",
    "\n",
    "# Convert the list of dictionaries to a pandas DataFrame\n",
    "df = pd.DataFrame(data_list)\n",
    "\n",
    "# Remove duplicate paintings\n",
    "df = df.drop_duplicates(subset='painting')\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(os.path.join(wikidata_dir, 'paintings.csv'),\n",
    "          index=False, quotechar=\"'\")\n",
    "print(f\"Saved {len(df)} paintings to 'paintings.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the wikidata-data directory if it doesn't exist\n",
    "wikidata_data_dir = os.path.join(os.getcwd(), 'wikidata-data')\n",
    "os.makedirs(wikidata_data_dir, exist_ok=True)\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(os.path.join(wikidata_dir, 'paintings.csv'), quotechar=\"'\")\n",
    "\n",
    "# Remove the last character from the iconclass code (e.g. '11H(PAUL)11' -> '11H(PAUL)')\n",
    "# And retrieve only the top 10 iconclasses\n",
    "df['iconclass'] = df['iconclass'].str.extract(r'([^\\)]+\\))')\n",
    "iconclass_counts = df['iconclass'].value_counts().head(10)\n",
    "print(iconclass_counts)\n",
    "\n",
    "# Filter the images to only include the top 10 iconclasses\n",
    "df_filtered = df[df['iconclass'].isin(iconclass_counts.index)]\n",
    "df_filtered = df_filtered.drop_duplicates(subset='image')\n",
    "\n",
    "df_filtered.to_csv(os.path.join(wikidata_dir, 'wikidata.csv'), index=False, quotechar=\"'\")\n",
    "print(f\"Saved {len(df_filtered)} paintings to 'wikidata.csv'\")\n",
    "\n",
    "iconclass_counts.to_csv(os.path.join(wikidata_data_dir, 'pre_classes.csv'), header=True)\n",
    "print(f\"Saved top 10 iconclass to 'pre_classes.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Images with Original Sizes\n",
    "\n",
    "In this cell, we download images while preserving their original sizes and aspect ratios. We only resize if the image is too large (to prevent decompression bomb attacks), but we maintain the aspect ratio in that case.\n",
    "\n",
    "Updates in this version:\n",
    "1. Changed output directory to `/home/ubuntu/gspinaci/LLM-test/dataset/wikidata/JPEGImages`\n",
    "2. Added verification after saving images\n",
    "3. Added tracking of failed downloads\n",
    "4. Enhanced logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from PIL import Image\n",
    "import io\n",
    "import numpy as np\n",
    "\n",
    "# Create the directory to save images if it doesn't exist\n",
    "# Changed the output directory to the requested path\n",
    "jpeg_images_dir = os.path.join('/home/ubuntu/gspinaci/LLM-test/dataset/wikidata/JPEGImages')\n",
    "os.makedirs(jpeg_images_dir, exist_ok=True)\n",
    "logging.info(f\"Output directory: {jpeg_images_dir}\")\n",
    "\n",
    "images_df = pd.read_csv(os.path.join(wikidata_dir, 'wikidata.csv'))\n",
    "logging.info(f\"Loaded {len(images_df)} images from CSV\")\n",
    "\n",
    "# Initialize lists to store the image data and image sizes\n",
    "image_data = []\n",
    "image_sizes = []\n",
    "failed_downloads = []\n",
    "\n",
    "# Enhanced function to download an image from a URL while preserving original size and aspect ratio\n",
    "def download_image(url, save_path, max_pixels=178956970, target_size=None):\n",
    "    \"\"\"\n",
    "    Download an image from a URL, preserving original size and aspect ratio.\n",
    "    \n",
    "    Args:\n",
    "        url: URL of the image to download\n",
    "        save_path: Path where the image will be saved\n",
    "        max_pixels: Maximum number of pixels allowed (width × height)\n",
    "        target_size: Optional (width, height) tuple for resizing (set to None to preserve original size)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (success, width, height) - success is a boolean, width and height are the dimensions of the saved image\n",
    "    \"\"\"\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "    \n",
    "    try:\n",
    "        # Download the image\n",
    "        logging.info(f\"Downloading {url}\")\n",
    "        response = requests.get(url, headers=headers, stream=True)\n",
    "        if response.status_code != 200:\n",
    "            logging.error(f\"Failed to download {url}: HTTP status code {response.status_code}\")\n",
    "            return False, 0, 0\n",
    "            \n",
    "        # Read the image data into memory\n",
    "        image_data = io.BytesIO()\n",
    "        for chunk in response.iter_content(1024):\n",
    "            image_data.write(chunk)\n",
    "        image_data.seek(0)\n",
    "        \n",
    "        # Open the image\n",
    "        with Image.open(image_data) as img:\n",
    "            # Convert to RGB if needed (handles PNG, RGBA, etc.)\n",
    "            if img.mode != 'RGB':\n",
    "                img = img.convert('RGB')\n",
    "                \n",
    "            width, height = img.size\n",
    "            num_pixels = width * height\n",
    "            original_size = f\"{width}x{height}\"\n",
    "            logging.info(f\"Original image size: {original_size} ({num_pixels} pixels)\")\n",
    "            \n",
    "            # Check if resizing is needed to prevent decompression bomb\n",
    "            if num_pixels > max_pixels:\n",
    "                # Calculate new dimensions while maintaining aspect ratio\n",
    "                ratio = width / height\n",
    "                if ratio > 1:\n",
    "                    new_width = int(np.sqrt(max_pixels * ratio))\n",
    "                    new_height = int(new_width / ratio)\n",
    "                else:\n",
    "                    new_height = int(np.sqrt(max_pixels / ratio))\n",
    "                    new_width = int(new_height * ratio)\n",
    "                \n",
    "                # Resize the image\n",
    "                img = img.resize((new_width, new_height), Image.LANCZOS)\n",
    "                width, height = new_width, new_height\n",
    "                logging.info(f\"Resized image from {original_size} to {width}x{height}\")\n",
    "            \n",
    "            # If a target size is specified, resize to that size\n",
    "            # Note: We're setting target_size=None to preserve original dimensions\n",
    "            if target_size:\n",
    "                img = img.resize(target_size, Image.LANCZOS)\n",
    "                width, height = target_size\n",
    "            \n",
    "            # Save the image\n",
    "            img.save(save_path, 'JPEG', quality=95)\n",
    "            \n",
    "            # Verify the image was saved successfully\n",
    "            if os.path.exists(save_path):\n",
    "                file_size = os.path.getsize(save_path)\n",
    "                if file_size > 0:\n",
    "                    logging.info(f\"Successfully saved {save_path} ({file_size} bytes)\")\n",
    "                    return True, width, height\n",
    "                else:\n",
    "                    logging.warning(f\"File saved but has zero size: {save_path}\")\n",
    "                    os.remove(save_path)  # Remove empty file\n",
    "                    return False, 0, 0\n",
    "            else:\n",
    "                logging.error(f\"Failed to save {save_path}\")\n",
    "                return False, 0, 0\n",
    "            \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to download {url}: {e}\")\n",
    "        return False, 0, 0\n",
    "\n",
    "# Process each image\n",
    "for idx, row in tqdm(images_df.iterrows(), total=len(images_df)):\n",
    "    if row['iconclass'] in iconclass_counts:\n",
    "        filename = row['painting'].split('/')[-1] + '.jpg'\n",
    "        save_path = os.path.join(jpeg_images_dir, filename)\n",
    "        \n",
    "        # Use the enhanced download function with target_size=None to preserve original dimensions\n",
    "        success, width, height = download_image(\n",
    "            row['image'], \n",
    "            save_path,\n",
    "            max_pixels=178956970,\n",
    "            target_size=None  # Set to None to preserve original size\n",
    "        )\n",
    "        \n",
    "        if success:\n",
    "            # Store the image dimensions\n",
    "            image_sizes.append((width, height))\n",
    "            \n",
    "            # Store the image and its class in the list\n",
    "            image_data.append({\n",
    "                'painting': row['painting'],\n",
    "                'image': row['image'],\n",
    "                'iconclass': row['iconclass'],\n",
    "                'width': width,\n",
    "                'height': height\n",
    "            })\n",
    "        else:\n",
    "            # Track failed downloads\n",
    "            failed_downloads.append({\n",
    "                'painting': row['painting'],\n",
    "                'image': row['image'],\n",
    "                'iconclass': row['iconclass']\n",
    "            })\n",
    "        \n",
    "        # Save the data to a JSON file every 50 images\n",
    "        if (idx + 1) % 50 == 0:\n",
    "            with open(os.path.join(wikidata_dir, 'wikidata_original.json'), 'w') as f:\n",
    "                json.dump(image_data, f)\n",
    "            logging.info(f\"Saved data for {len(image_data)} images to JSON\")\n",
    "\n",
    "# Save any remaining data to the JSON file\n",
    "with open(os.path.join(wikidata_dir, 'wikidata_original.json'), 'w') as f:\n",
    "    json.dump(image_data, f)\n",
    "\n",
    "# Save failed downloads to a separate file\n",
    "with open(os.path.join(wikidata_dir, 'failed_downloads.json'), 'w') as f:\n",
    "    json.dump(failed_downloads, f)\n",
    "logging.info(f\"Saved {len(failed_downloads)} failed downloads to JSON\")\n",
    "\n",
    "print(\"Image download complete.\")\n",
    "print(f\"Successfully downloaded: {len(image_data)} images\")\n",
    "print(f\"Failed downloads: {len(failed_downloads)} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Image Statistics\n",
    "\n",
    "Now we'll calculate statistics on the image dimensions to compare with the target values from the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate statistics on image dimensions\n",
    "if image_sizes:\n",
    "    widths = [size[0] for size in image_sizes]\n",
    "    heights = [size[1] for size in image_sizes]\n",
    "    \n",
    "    width_mean = np.mean(widths)\n",
    "    width_std = np.std(widths)\n",
    "    height_mean = np.mean(heights)\n",
    "    height_std = np.std(heights)\n",
    "    \n",
    "    print(f\"Image Statistics:\")\n",
    "    print(f\"Width: {width_mean:.2f} ± {width_std:.2f} pixels\")\n",
    "    print(f\"Height: {height_mean:.2f} ± {height_std:.2f} pixels\")\n",
    "    print(f\"Target from paper: Width: 778.84 ± 198.74, Height: 669.36 ± 174.18\")\n",
    "    print(f\"\\nTotal images: {len(image_sizes)}\")\n",
    "    print(f\"Min width: {min(widths)}, Max width: {max(widths)}\")\n",
    "    print(f\"Min height: {min(heights)}, Max height: {max(heights)}\")\n",
    "    print(f\"Failed downloads: {len(failed_downloads)}\")\n",
    "    \n",
    "    # Save statistics to a file\n",
    "    with open(os.path.join(wikidata_data_dir, 'image_statistics.txt'), 'w') as f:\n",
    "        f.write(f\"Image Statistics:\\n\")\n",
    "        f.write(f\"Width: {width_mean:.2f} ± {width_std:.2f} pixels\\n\")\n",
    "        f.write(f\"Height: {height_mean:.2f} ± {height_std:.2f} pixels\\n\")\n",
    "        f.write(f\"Target from paper: Width: 778.84 ± 198.74, Height: 669.36 ± 174.18\\n\")\n",
    "        f.write(f\"\\nTotal images: {len(image_sizes)}\\n\")\n",
    "        f.write(f\"Min width: {min(widths)}, Max width: {max(widths)}\\n\")\n",
    "        f.write(f\"Min height: {min(heights)}, Max height: {max(heights)}\\n\")\n",
    "        f.write(f\"Failed downloads: {len(failed_downloads)}\\n\")\n",
    "    \n",
    "    logging.info(\"Saved image statistics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Test and Ground Truth Files\n",
    "\n",
    "Finally, we'll create the test and ground truth files for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test and ground truth files\n",
    "test_images = []\n",
    "ground_truth = []\n",
    "\n",
    "# Iterate over each object in the image data\n",
    "for item in image_data:\n",
    "    # Extract the image filename\n",
    "    image_filename = item['painting'].replace('http://www.wikidata.org/entity/', '')\n",
    "    image_path = os.path.join(jpeg_images_dir, f'{image_filename}.jpg')\n",
    "    \n",
    "    # Check if the image exists in JPEGImages directory\n",
    "    if os.path.exists(image_path):\n",
    "        # Add the image filename to the test file list\n",
    "        test_images.append(image_filename)\n",
    "        \n",
    "        # Add the object to the ground truth list\n",
    "        ground_truth.append({\n",
    "            'item': image_filename,\n",
    "            'class': item['iconclass'],\n",
    "            'width': item.get('width', 0),\n",
    "            'height': item.get('height', 0)\n",
    "        })\n",
    "\n",
    "# Write the test images to 2_test_original.txt\n",
    "with open(os.path.join(wikidata_data_dir, '2_test_original.txt'), 'w') as f:\n",
    "    for image in test_images:\n",
    "        f.write(f\"{image}\\n\")\n",
    "\n",
    "# Write the ground truth data to 2_ground_truth_original.json\n",
    "with open(os.path.join(wikidata_data_dir, '2_ground_truth_original.json'), 'w') as f:\n",
    "    json.dump(ground_truth, f)\n",
    "\n",
    "print(f\"Files 2_test_original.txt and 2_ground_truth_original.json have been created.\")\n",
    "print(f\"Downloaded {len(test_images)} images with original sizes and aspect ratios.\")\n",
    "print(f\"Failed downloads: {len(failed_downloads)}\")\n",
    "logging.info(f\"Process complete. Downloaded {len(test_images)} images, failed {len(failed_downloads)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of Image Sizes\n",
    "\n",
    "Let's visualize the distribution of image sizes to better understand how they compare to the target values from the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set up the figure\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plot width distribution\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.histplot(widths, kde=True)\n",
    "plt.axvline(x=width_mean, color='r', linestyle='--', label=f'Mean: {width_mean:.2f}')\n",
    "plt.axvline(x=778.84, color='g', linestyle='--', label='Target Mean: 778.84')\n",
    "plt.title('Width Distribution')\n",
    "plt.xlabel('Width (pixels)')\n",
    "plt.ylabel('Count')\n",
    "plt.legend()\n",
    "\n",
    "# Plot height distribution\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.histplot(heights, kde=True)\n",
    "plt.axvline(x=height_mean, color='r', linestyle='--', label=f'Mean: {height_mean:.2f}')\n",
    "plt.axvline(x=669.36, color='g', linestyle='--', label='Target Mean: 669.36')\n",
    "plt.title('Height Distribution')\n",
    "plt.xlabel('Height (pixels)')\n",
    "plt.ylabel('Count')\n",
    "plt.legend()\n",
    "\n",
    "# Plot width vs height scatter\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.scatter(widths, heights, alpha=0.5)\n",
    "plt.scatter(778.84, 669.36, color='r', s=100, marker='*', label='Target Mean')\n",
    "plt.scatter(width_mean, height_mean, color='g', s=100, marker='o', label='Actual Mean')\n",
    "plt.title('Width vs Height')\n",
    "plt.xlabel('Width (pixels)')\n",
    "plt.ylabel('Height (pixels)')\n",
    "plt.legend()\n",
    "\n",
    "# Plot aspect ratio distribution\n",
    "aspect_ratios = [w/h for w, h in zip(widths, heights)]\n",
    "plt.subplot(2, 2, 4)\n",
    "sns.histplot(aspect_ratios, kde=True)\n",
    "plt.axvline(x=np.mean(aspect_ratios), color='r', linestyle='--', \n",
    "            label=f'Mean: {np.mean(aspect_ratios):.2f}')\n",
    "plt.axvline(x=778.84/669.36, color='g', linestyle='--', \n",
    "            label=f'Target: {778.84/669.36:.2f}')\n",
    "plt.title('Aspect Ratio Distribution')\n",
    "plt.xlabel('Aspect Ratio (width/height)')\n",
    "plt.ylabel('Count')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(wikidata_data_dir, 'image_size_distribution.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Failed Downloads\n",
    "\n",
    "Let's examine the failed downloads to understand why they failed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the failed downloads\n",
    "if os.path.exists(os.path.join(wikidata_dir, 'failed_downloads.json')):\n",
    "    with open(os.path.join(wikidata_dir, 'failed_downloads.json'), 'r') as f:\n",
    "        failed_downloads = json.load(f)\n",
    "    \n",
    "    print(f\"Total failed downloads: {len(failed_downloads)}\")\n",
    "    \n",
    "    # Display the first few failed downloads\n",
    "    if failed_downloads:\n",
    "        print(\"\\nSample of failed downloads:\")\n",
    "        for i, item in enumerate(failed_downloads[:5]):\n",
    "            print(f\"\\n{i+1}. Painting: {item['painting']}\")\n",
    "            print(f\"   Image URL: {item['image']}\")\n",
    "            print(f\"   Iconclass: {item['iconclass']}\")\n",
    "    \n",
    "    # Group by iconclass\n",
    "    iconclass_failures = {}\n",
    "    for item in failed_downloads:\n",
    "        iconclass = item['iconclass']\n",
    "        if iconclass in iconclass_failures:\n",
    "            iconclass_failures[iconclass] += 1\n",
    "        else:\n",
    "            iconclass_failures[iconclass] = 1\n",
    "    \n",
    "    print(\"\\nFailures by iconclass:\")\n",
    "    for iconclass, count in sorted(iconclass_failures.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"{iconclass}: {count} failures\")\n",
    "else:\n",
    "    print(\"No failed downloads file found.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
