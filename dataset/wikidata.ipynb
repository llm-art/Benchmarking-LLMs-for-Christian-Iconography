{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/gspinaci/LLM-test/venv/lib/python3.10/site-packages/SPARQLWrapper/Wrapper.py:399: RuntimeWarning: invalid update method 'GET'\n",
      "  warnings.warn(\"invalid update method '%s'\" % method, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 2180 paintings to 'paintings.csv'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "wikidata_dir = os.path.join(os.getcwd(), 'wikidata')\n",
    "os.makedirs(wikidata_dir, exist_ok=True)\n",
    "\n",
    "# Define the SPARQL endpoint and the query\n",
    "sparql = SPARQLWrapper(\"https://query.wikidata.org/sparql\")\n",
    "query = \"\"\"\n",
    "SELECT ?painting ?image ?iconclass WHERE {\n",
    "  ?painting wdt:P31 wd:Q3305213;        # instance of painting\n",
    "           wdt:P1257 ?iconclass.        # has an Iconclass code\n",
    "  ?painting wdt:P18 ?image.             # image filename if available\n",
    "  FILTER(strstarts(?iconclass, '11H'))\n",
    "}\n",
    "\"\"\"\n",
    "sparql.setQuery(query)\n",
    "sparql.setReturnFormat(JSON)\n",
    "\n",
    "# Execute the query and convert the result to a pandas DataFrame\n",
    "sparql.setRequestMethod('GET')\n",
    "results = sparql.query().convert()\n",
    "data = results['results']['bindings']\n",
    "\n",
    "# Extract the relevant fields and store them in a list of dictionaries\n",
    "data_list = []\n",
    "for item in data:\n",
    "  data_list.append({\n",
    "    'painting': item['painting']['value'],\n",
    "    'image': item['image']['value'],\n",
    "    'iconclass': item['iconclass']['value']\n",
    "  })\n",
    "\n",
    "# Convert the list of dictionaries to a pandas DataFrame\n",
    "df = pd.DataFrame(data_list)\n",
    "\n",
    "# Remove duplicate paintings\n",
    "df = df.drop_duplicates(subset='painting')\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(os.path.join(wikidata_dir, 'paintings.csv'),\n",
    "          index=False, quotechar=\"'\")\n",
    "print(f\"Saved {len(df)} paintings to 'paintings.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iconclass\n",
      "11HH(MARY MAGDALENE)     177\n",
      "11H(JOHN THE BAPTIST)    131\n",
      "11H(JEROME)               78\n",
      "11HH(CATHERINE)           76\n",
      "11H(PETER)                68\n",
      "11H(JOHN)                 51\n",
      "11H(FRANCIS)              40\n",
      "11H(ANTONY ABBOT)         38\n",
      "11H(JOSEPH)               35\n",
      "11H(PAUL)                 31\n",
      "Name: count, dtype: int64\n",
      "Saved 724 paintings to 'wikidata.csv'\n",
      "Saved top 10 iconclass to 'pre_classes.csv'\n"
     ]
    }
   ],
   "source": [
    "# Create the wikidata-data directory if it doesn't exist\n",
    "wikidata_data_dir = os.path.join(os.getcwd(), 'wikidata-data')\n",
    "os.makedirs(wikidata_data_dir, exist_ok=True)\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(os.path.join(wikidata_dir, 'paintings.csv'), quotechar=\"'\")\n",
    "\n",
    "# Remove the last character from the iconclass code (e.g. '11H(PAUL)11' -> '11H(PAUL)')\n",
    "# And retrieve only the top 10 iconclasses\n",
    "df['iconclass'] = df['iconclass'].str.extract(r'([^\\)]+\\))')\n",
    "iconclass_counts = df['iconclass'].value_counts().head(10)\n",
    "print(iconclass_counts)\n",
    "\n",
    "# Filter the images to only include the top 10 iconclasses\n",
    "df_filtered = df[df['iconclass'].isin(iconclass_counts.index)]\n",
    "df_filtered = df_filtered.drop_duplicates(subset='image')\n",
    "\n",
    "df_filtered.to_csv(os.path.join(wikidata_dir, 'wikidata.csv'), index=False, quotechar=\"'\")\n",
    "print(f\"Saved {len(df_filtered)} paintings to 'wikidata.csv'\")\n",
    "\n",
    "iconclass_counts.to_csv(os.path.join(wikidata_data_dir, 'pre_classes.csv'), header=True)\n",
    "print(f\"Saved top 10 iconclass to 'pre_classes.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 59/724 [00:51<07:44,  1.43it/s]/home/ubuntu/gspinaci/LLM-test/venv/lib/python3.10/site-packages/PIL/Image.py:3406: DecompressionBombWarning: Image size (100858534 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      " 21%|██        | 149/724 [02:19<33:14,  3.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download http://commons.wikimedia.org/wiki/Special:FilePath/Conversion%20of%20Paul%20%28Bruegel%29.jpg: Image size (1078805910 pixels) exceeds limit of 178956970 pixels, could be decompression bomb DOS attack.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 220/724 [03:47<08:25,  1.00s/it]/home/ubuntu/gspinaci/LLM-test/venv/lib/python3.10/site-packages/PIL/Image.py:3406: DecompressionBombWarning: Image size (93228510 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      " 37%|███▋      | 271/724 [04:38<07:03,  1.07it/s]/home/ubuntu/gspinaci/LLM-test/venv/lib/python3.10/site-packages/PIL/Image.py:3406: DecompressionBombWarning: Image size (115258848 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      " 38%|███▊      | 277/724 [04:47<08:08,  1.09s/it]/home/ubuntu/gspinaci/LLM-test/venv/lib/python3.10/site-packages/PIL/Image.py:3406: DecompressionBombWarning: Image size (129804506 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      " 40%|███▉      | 289/724 [05:35<1:02:13,  8.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download http://commons.wikimedia.org/wiki/Special:FilePath/Hm%2054-oberrheinischer%20meister%20um%20141020-das%20paradiesgartlein-1410.png: Image size (346536264 pixels) exceeds limit of 178956970 pixels, could be decompression bomb DOS attack.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 391/724 [07:44<07:30,  1.35s/it]  /home/ubuntu/gspinaci/LLM-test/venv/lib/python3.10/site-packages/PIL/Image.py:3406: DecompressionBombWarning: Image size (118938834 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      " 57%|█████▋    | 414/724 [08:26<14:55,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download http://commons.wikimedia.org/wiki/Special:FilePath/El%20Descendimiento%2C%20by%20Rogier%20van%20der%20Weyden%2C%20from%20Prado%20in%20Google%20Earth.jpg: Image size (698310000 pixels) exceeds limit of 178956970 pixels, could be decompression bomb DOS attack.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 558/724 [11:25<09:12,  3.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download http://commons.wikimedia.org/wiki/Special:FilePath/Giovanni%20Bellini%20-%20Saint%20Francis%20in%20the%20Desert%20-%20Google%20Art%20Project.jpg: Image size (789570000 pixels) exceeds limit of 178956970 pixels, could be decompression bomb DOS attack.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 580/724 [11:53<03:02,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download http://commons.wikimedia.org/wiki/Special:FilePath/Ambrosius%20Benson%20-%20Hieronymus%20-%20MMB.0043%20-%20Museum%20Mayer%20van%20den%20Bergh.tiff: can't concat tuple to bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 581/724 [11:55<03:44,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download http://commons.wikimedia.org/wiki/Special:FilePath/Atelier%20van%20Joachim%20Patinir%20-%20Rotslandschap%20met%20Hieronymus%20-%20MMB.0030%20-%20Museum%20Mayer%20van%20den%20Bergh.tiff: can't concat tuple to bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 649/724 [13:12<01:20,  1.07s/it]/home/ubuntu/gspinaci/LLM-test/venv/lib/python3.10/site-packages/PIL/Image.py:3406: DecompressionBombWarning: Image size (141250812 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      " 95%|█████████▍| 685/724 [14:08<00:39,  1.02s/it]/home/ubuntu/gspinaci/LLM-test/venv/lib/python3.10/site-packages/PIL/Image.py:3406: DecompressionBombWarning: Image size (97199074 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "100%|██████████| 724/724 [14:52<00:00,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image download complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from PIL import Image\n",
    "import io\n",
    "import numpy as np\n",
    "\n",
    "# Create the directory to save images if it doesn't exist\n",
    "jpeg_images_dir = os.path.join(wikidata_dir, 'JPEGImages')\n",
    "os.makedirs(jpeg_images_dir, exist_ok=True)\n",
    "\n",
    "images_df = pd.read_csv(os.path.join(wikidata_dir, 'wikidata.csv'))\n",
    "\n",
    "# Initialize a list to store the image data\n",
    "image_data = []\n",
    "\n",
    "# Define target size - 512x512 is a good balance for most models\n",
    "# This size works well for the 512 model and the 384 models can downscale during processing\n",
    "target_size = (512, 512)\n",
    "\n",
    "# Enhanced function to download and resize an image from a URL\n",
    "def download_image(url, save_path, max_pixels=178956970, target_size=(512, 512)):\n",
    "  \"\"\"\n",
    "  Download and resize an image from a URL.\n",
    "  \n",
    "  Args:\n",
    "    url: URL of the image to download\n",
    "    save_path: Path where the image will be saved\n",
    "    max_pixels: Maximum number of pixels allowed (width × height)\n",
    "    target_size: Optional (width, height) tuple for resizing\n",
    "  \"\"\"\n",
    "  headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "  \n",
    "  try:\n",
    "    # Download the image\n",
    "    response = requests.get(url, headers=headers, stream=True)\n",
    "    if response.status_code != 200:\n",
    "      print(f\"Failed to download {url}: HTTP status code {response.status_code}\")\n",
    "      return False\n",
    "      \n",
    "    # Read the image data into memory\n",
    "    image_data = io.BytesIO()\n",
    "    for chunk in response.iter_content(1024):\n",
    "      image_data.write(chunk)\n",
    "    image_data.seek(0)\n",
    "    \n",
    "    # Open the image\n",
    "    with Image.open(image_data) as img:\n",
    "      # Convert to RGB if needed (handles PNG, RGBA, etc.)\n",
    "      if img.mode != 'RGB':\n",
    "        img = img.convert('RGB')\n",
    "        \n",
    "      width, height = img.size\n",
    "      num_pixels = width * height\n",
    "      original_size = f\"{width}x{height}\"\n",
    "      \n",
    "      # Check if resizing is needed to prevent decompression bomb\n",
    "      if num_pixels > max_pixels:\n",
    "        # Calculate new dimensions while maintaining aspect ratio\n",
    "        ratio = width / height\n",
    "        if ratio > 1:\n",
    "          new_width = int(np.sqrt(max_pixels * ratio))\n",
    "          new_height = int(new_width / ratio)\n",
    "        else:\n",
    "          new_height = int(np.sqrt(max_pixels / ratio))\n",
    "          new_width = int(new_height * ratio)\n",
    "        \n",
    "        # Resize the image\n",
    "        img = img.resize((new_width, new_height), Image.LANCZOS)\n",
    "      \n",
    "      # If a target size is specified, resize to that size\n",
    "      if target_size:\n",
    "        img = img.resize(target_size, Image.LANCZOS)\n",
    "      \n",
    "      # Save the image\n",
    "      img.save(save_path, 'JPEG', quality=95)\n",
    "      return True\n",
    "      \n",
    "  except Exception as e:\n",
    "    print(f\"Failed to download {url}: {e}\")\n",
    "    return False\n",
    "\n",
    "# Modify the filename to use the painting column (which is a URI)\n",
    "for idx, row in tqdm(images_df.iterrows(), total=len(images_df)):\n",
    "  if row['iconclass'] in iconclass_counts:\n",
    "    filename = row['painting'].split('/')[-1] + '.jpg'\n",
    "    save_path = os.path.join(jpeg_images_dir, filename)\n",
    "    \n",
    "    # Use the enhanced download function\n",
    "    success = download_image(\n",
    "      row['image'], \n",
    "      save_path,\n",
    "      max_pixels=178956970,  # PIL's default limit\n",
    "      target_size=target_size\n",
    "    )\n",
    "    \n",
    "    if success:\n",
    "      # Store the image and its class in the list\n",
    "      image_data.append({\n",
    "        'painting': row['painting'],\n",
    "        'image': row['image'],\n",
    "        'iconclass': row['iconclass']\n",
    "      })\n",
    "    \n",
    "    # Save the data to a JSON file every 50 images\n",
    "    if (idx + 1) % 50 == 0:\n",
    "      with open(os.path.join(wikidata_dir, 'wikidata.json'), 'w') as f:\n",
    "        json.dump(image_data, f)\n",
    "\n",
    "# Save any remaining data to the JSON file\n",
    "with open(os.path.join(wikidata_dir, 'wikidata.json'), 'w') as f:\n",
    "  json.dump(image_data, f)\n",
    "\n",
    "print(\"Image download complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['11HH(MARY MAGDALENE)', '11H(JOHN THE BAPTIST)', '11H(JEROME)', '11HH(CATHERINE)', '11H(PETER)', '11H(JOHN)', '11H(FRANCIS)', '11H(ANTONY ABBOT)', '11H(JOSEPH)', '11H(PAUL)']\n",
      "Files 2_test.txt and 2_ground_truth.json have been created.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "wikidata_dir = os.path.join(os.getcwd(), 'wikidata')\n",
    "wikidata_data_dir = os.path.join(os.getcwd(), 'wikidata-data')\n",
    "\n",
    "jpeg_images_dir = os.path.join(wikidata_dir, 'JPEGImages')\n",
    "\n",
    "top_classes_df = pd.read_csv(os.path.join(wikidata_data_dir, 'pre_classes.csv'))\n",
    "top_classes = top_classes_df['iconclass'].to_list()\n",
    "\n",
    "print(top_classes)\n",
    "\n",
    "# Read the paintings.csv file\n",
    "image_data = pd.read_csv(os.path.join(wikidata_dir, 'wikidata.csv'), quotechar=\"'\")\n",
    "\n",
    "# Initialize lists to store the data\n",
    "test_images = []\n",
    "ground_truth = []\n",
    "\n",
    "# Iterate over each object in the JSON data\n",
    "for item in image_data.itertuples():\n",
    "  # Extract the image filename\n",
    "  image_filename = item.painting.replace('http://www.wikidata.org/entity/', '')\n",
    "  image_path = os.path.join(jpeg_images_dir, f'{image_filename}.jpg')\n",
    "\n",
    "  # Check if the image exists in JPEGImages directory\n",
    "  if os.path.exists(image_path):\n",
    "    # Add the image filename to the test file list\n",
    "    test_images.append(image_filename)\n",
    "    \n",
    "    # Add the object to the ground truth list\n",
    "    ground_truth.append({\n",
    "      'item': image_filename,\n",
    "      'class': item.iconclass\n",
    "    })\n",
    "\n",
    "# Write the test images to 2_test.txt\n",
    "with open(os.path.join(wikidata_data_dir, '2_test.txt'), 'w') as f:\n",
    "  for image in test_images:\n",
    "    f.write(f\"{image}\\n\")\n",
    "\n",
    "# Write the ground truth data to 2_ground_truth.json\n",
    "with open(os.path.join(wikidata_data_dir, '2_ground_truth.json'), 'w') as f:\n",
    "  json.dump(ground_truth, f)\n",
    "\n",
    "print(\"Files 2_test.txt and 2_ground_truth.json have been created.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
